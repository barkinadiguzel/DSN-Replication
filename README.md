# ğŸŒ¸ DSN Replication â€“ Deeply Supervised Neural Networks

This repository provides a **PyTorch-based replication** of the  
**Deeply-Supervised Nets (DSN) â€“ Improving Feature Learning with Hidden Layer Supervision**.

The focus is **understanding how companion objectives enhance hidden layer discriminativeness**  
rather than purely optimizing for state-of-the-art accuracy.

- Backbone CNN with **companion heads** ğŸ¾  
- Companion objectives supervise hidden layers early ğŸ„  
- Squared hinge loss ensures robust **feature learning** ğŸ  
- Total loss balances output + hidden layers âœ¨  

**Paper reference:** [DSN â€“ Lee et al., 2015](https://arxiv.org/abs/1409.5185) ğŸŒ·

---

## ğŸŒŒ Overview â€“ DSN Architecture

![DSN Example](images/figmix.jpg)

### ğŸš€ High-level Pipeline

1. **Input image**

```math
X \in \mathbb{R}^{C \times H \times W}, \quad Z^{(0)} = X
```

2. **Backbone layers**

```math
Q^{(m)} = W^{(m)} * Z^{(m-1)}, \quad Z^{(m)} = f(Q^{(m)}), \quad m=1..M
```

3. **Companion outputs for hidden layers**

```math
\hat{y}^{(m)} = \phi(Z^{(m)}, w^{(m)}), \quad m=1..M-1
```

4. **Final output layer**

```math 
\hat{y}^{\text{out}} = \phi(Z^{(M)}, w^{\text{out}})
```

5. **Total objective**

```math
\mathcal{L}_{\text{total}} = 
\underbrace{ \|w^{\text{out}}\|^2 + L(W, w^{\text{out}}) }_{\text{output loss}} +
\sum_{m=1}^{M-1} \alpha_m \underbrace{ [ \|w^{(m)}\|^2 + \ell(W, w^{(m)}) - \gamma ]_+}_{\text{companion loss}}
```

---

## ğŸ§  What the Model Learns

- **Backbone**: hierarchical feature extraction ğŸŒ¿  
- **Companion heads**: supervise hidden layers â†’ discriminative features early ğŸ¥  
- **Squared hinge loss**:

```python
loss = torch.mean(torch.clamp(1 - logits*(2*target_onehot - 1), min=0)**2)
```
- **Total los**s: weighted sum of output + companion losses ğŸ’«

- **Threshold Î³**: companion loss only affects learning if above threshold
  
---
## ğŸ“¦ Repository Structure

```bash
BranchyNet-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_block.py            # Reusable Conv + activation block for feature extraction
â”‚   â”‚   â”œâ”€â”€ activation.py            # Activation functions (ReLU, LeakyReLU, Sigmoid, etc.)
â”‚   â”‚   â”œâ”€â”€ normalization.py         # Normalization layers (BatchNorm, LayerNorm)
â”‚   â”‚   â””â”€â”€ pooling.py               # Pooling operations (MaxPool, AvgPool)
â”‚   â”‚
â”‚   â”œâ”€â”€ exits/
â”‚   â”‚   â”œâ”€â”€ entropy.py               # Entropy computation for early-exit confidence
â”‚   â”‚   â””â”€â”€ exit_decision.py         # Threshold-based early-exit decision logic
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â”œâ”€â”€ backbone_block.py        # Backbone network blocks (VGG/ResNet-style)
â”‚   â”‚   â”œâ”€â”€ branch_block.py          # Side branch attached to the backbone
â”‚   â”‚   â””â”€â”€ classifier_head.py       # Lightweight classifier for each exit
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ branchynet.py            # Full model: backbone with multiple early exits with Forward pass with early-exit control flow
â”‚   â”‚
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚   â””â”€â”€ joint_loss.py            # Weighted sum of losses from all exits (theoretical)
â”‚   â”‚
â”‚   â””â”€â”€ config.py                    # Number of exits, entropy thresholds, loss weights
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg                 
â”‚
â”œâ”€â”€ requirements.txt                
â””â”€â”€ README.md                     

```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
